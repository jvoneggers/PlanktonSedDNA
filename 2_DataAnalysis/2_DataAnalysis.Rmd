---
title: "Phytoplankton and zooplankton sedimentary DNA from Western United States"
author: "Jordan Von Eggers"
date: "`r Sys.Date()`"
output: pdf_document
editor_options: 
  chunk_output_type: console
---

# Load packages 
```{r}
require(tidyverse)
require(ggh4x)
require(lubridate)
require(phyloseq)


  custom_theme <- function() {
  theme_bw() +
    theme(
      text = element_text(color = "black", size = 13),
      axis.text = element_text(color = "black",size = 13),
      axis.title = element_text(color = "black",size = 13),
      axis.title.x = element_text(margin = margin(t = 10)),
      axis.title.y = element_text(margin = margin(r = 10)),
      plot.title = element_text(hjust = 0.5, size = 14, face = "bold"),
      plot.subtitle = element_text(hjust = 0.5, size = 12),
      legend.text = element_text(size = 13),
      legend.title = element_text(size = 13,hjust = 0.5),
      strip.text = element_text(size = 13, color = "black"))}
  
source("1_LoadData/1_CreatePhyloseq_ZOTUexact_TAX80.R")
```


# 1. Compile sample metadata
```{r}
# biogenic silica and C-H measurements

# here I am reading in the sample list to help create unique biogenic silica error and C-H error measurements per lake. 
sample_list <- read.csv("/Users/jordanscheibe/Library/CloudStorage/OneDrive-UniversityofWyoming/LakeSedDNA/Data/Core and sample information/Master/2024-10-10_MASTER_sample_id_list.csv") 
bisi<-read.csv("/Users/jordanscheibe/Library/CloudStorage/OneDrive-UniversityofWyoming/LakeSedDNA/Data/Biogenic silica/results/MASTER_BiogenicSilica_FTIR_measurements.csv", header = T) %>% left_join(.,sample_list, by=join_by(sample_id)) %>% select(sample_id, replicate, FTIR_measurement, FTIR_value, lake_name) 

bisi <- bisi %>%
        group_by(sample_id) %>%
        filter(all(c("R2", "R3") %in% replicate)) %>%
        ungroup() %>%
        group_by(sample_id, lake_name, FTIR_measurement) %>%
        summarize(
                range_FTIR_value = max(FTIR_value) - min(FTIR_value),
                .groups = 'drop' ) %>%
        group_by(lake_name, FTIR_measurement) %>%
        summarize(
                mean_range_FTIR_value = mean(range_FTIR_value),
                max_range_FTIR_value = max(range_FTIR_value),
                .groups = 'drop' ) %>%
        # merge summary back into data
        left_join(bisi,., by=join_by(lake_name, FTIR_measurement)) %>% 
        # get the average for each sample and measurement for the replicates
        group_by(sample_id,FTIR_measurement) %>% mutate(mean_FTIR_value=mean(FTIR_value)) %>% ungroup() %>% select(-c(FTIR_value, replicate)) %>% distinct(.) %>% left_join(.,sample_list, by=join_by(sample_id, lake_name)) %>% select(-c(lake_name,full_sample_code,lake_code,mountain_code,core,drive,top_cm,bottom_cm,mid_cm,rep_or_note)) %>%
  pivot_wider(
    names_from = FTIR_measurement, 
    values_from = c(mean_range_FTIR_value, max_range_FTIR_value, mean_FTIR_value),
    names_sep = "_"
  ) %>%  rename_with(~ gsub("FTIR_value_", "", .)) %>% rename_with(~ gsub("BSi", "BiSi", .))


table(bisi$sample_id%in%sample_list$sample_id) # all T
metadata<-left_join(sample_list,bisi, join_by(sample_id)); rm(sample_list); rm(bisi)

# 210Pb data
pb<-read.csv("/Users/jordanscheibe/Library/CloudStorage/OneDrive-UniversityofWyoming/LakeSedDNA/Data/210Pb dating/210lead results/MASTER_2024-09-24_210Pb_2020_2021_cores.csv", header=T)
table(pb$sample_id%in%metadata$sample_id) # all T
metadata<-left_join(metadata,pb, join_by(sample_id)); rm(pb)

# stable isotope data
iso<-read.csv("/Users/jordanscheibe/Library/CloudStorage/OneDrive-UniversityofWyoming/LakeSedDNA/Data/Stable isotopes/SIF_results/MASTER_stable_isotopes_2020_2021_cores.csv")
table(iso$sample_id%in%metadata$sample_id) # all T
metadata<-left_join(metadata,iso, join_by(sample_id)); rm(iso)

# LOI data
loi<-read.csv("/Users/jordanscheibe/Library/CloudStorage/OneDrive-UniversityofWyoming/LakeSedDNA/Data/Subsampling and LOI/MASTER_loss_on_ignition_2015_2019-2021_cores.csv") %>% filter(mountain_lake_code%in%c("SNOW20_LOST20_1A_1G", "SNOW20_SCOT20_1A_1G","WIND20_BLRO20_1A_1G", "WIND20_CANY20_1A_1G" ,"WIND20_EYRI20_1A_1G", "WIND20_FOOT20_1A_1G","WIND20_LIGH20_1A_1G", "MORA21_CRES21_1A_1G", "MORA21_LOUI21_1A_1G",
"NOCA21_MONO21_1A_1G", "SEKI21_BULL21_2A_1G", "YOSE21_MIGA21_1A_1G", "YOSE21_SKEL21_1A_1G", "YOSE21_SOLD21_1A_1G")) %>% select(-c(top_cm,bottom_cm,full_sample_code, year_cored))
table(loi$sample_id%in%metadata$sample_id) # all T
setdiff(loi$sample_id, metadata$sample_id) # these two samples were combined into SOLD21_0to1 so OK!
metadata<-left_join(metadata,loi, join_by(sample_id)); rm(loi)

# lake data
lake<-read.csv("/Users/jordanscheibe/Library/CloudStorage/OneDrive-UniversityofWyoming/LakeSedDNA/Data/Core and sample information/Master/2024-10-10_MASTER_lake_data.csv")
table(lake$lake_name%in%metadata$lake_name) # all T
metadata<-left_join(metadata,lake, join_by(lake_name)); rm(lake)

# remove lakes not included in this study
metadata<-metadata %>% filter(!lake_name %in% c("Green Park Lake","Solitude", "Delta", "Lake of the Crags"))
```
662 samples from 14 lakes

# 2. Model 210Pb dating

Here I am:
1. using Loess line to interpolate samples that did not have 210Pb measured but were within the range of 210Pb dating measurements
2. extrapolating past the range of 210Pb dating using the dry mass accumulation rate (DMAR)
3. creating three final columns that are a combination of the measured 210Pb dates, the interpolated dates, and extrapolated dates. 


```{r}
# indicate if 210Pb was measured for the sample
pb <- metadata %>% mutate(lead210_measured=ifelse(is.na(lead210_measured)==T,"n","y"))

# remove the bottom Soldier sample dating back around 1850 because the error is ~350 years and is messing up extrapolation. 
pb<-pb %>%  mutate(across(c(cum_dry_mass_g_cm2, unsup_210Pb_pCi_g, error_of_unsup_Pb_sd,age_base_of_interval_yr,error_of_age_sd,date_mid_AD,date_base_AD,DMAR_g_cm2_yr,error_DMAR_sd),
                ~ ifelse(sample_id == "SOLD21_17", NA, .)))

# calculate density and rho for using with dry mass accumulation rate (DMAR)
pb <- pb %>%
        mutate(dry_perc=100-water_perc,
               wet_density_g_cm3=wet_wt_g/volume_cm3,
               all_rho_g_cm3=round(dry_perc/(dry_perc/2.4+water_perc),digits=4))
# only wet density for Wind River and Snowy cores (maybe a few CA cores)

# indicate if the part of the core is exact measurement, interpolated, or extrapolated
# first calculate the deepest 210Pb measurement
max_bottom_cm <- pb %>%
  group_by(lake_name) %>%
  filter(is.na(date_mid_AD) == F) %>%
  summarize(max_bottom_cm_210Pb_dated = max(bottom_cm))%>% ungroup()

# then calculate the shallowest 210Pb measurement (specifically for Soldier)
min_bottom_cm <- pb %>%
  group_by(lake_name) %>%
  filter(is.na(date_mid_AD) == F) %>%
  summarize(min_bottom_cm_210Pb_dated = min(bottom_cm))%>% ungroup()


# merge with main data frame
pb<- left_join(pb,max_bottom_cm, by=join_by(lake_name)); rm(max_bottom_cm) # max
pb<- left_join(pb,min_bottom_cm, by=join_by(lake_name)); rm(min_bottom_cm) # min

pb <- pb %>% group_by(lake_name) %>%
        mutate(date_type=ifelse(bottom_cm>max_bottom_cm_210Pb_dated,"extrapolated",
                                ifelse(lead210_measured=="y","measured",
                                        ifelse(lead210_measured=="n","interpolated",NA )))) %>% ungroup()

# use Loess line to predict interpolated values (middle of sample)

interpolated_dates_mid <- pb %>%
        group_by(lake_name) %>%
        do({
        loess_fit_mean <- loess(date_mid_AD ~ mid_cm, data = .)
        loess_fit_sd <- loess(error_of_age_sd ~ mid_cm, data = .)
        new_data <- data.frame(mid_cm = seq(unique(.$min_bottom_cm_210Pb_dated)- 0.25, unique(.$max_bottom_cm_210Pb_dated) - 0.25, by = 0.5))
        new_data$interpolated_date_mid_AD <- round(predict(loess_fit_mean, newdata = new_data),digits=2)
        new_data$interpolated_error_of_age_sd <- round(predict(loess_fit_sd, newdata = new_data), digits=3)
        new_data$lake_name <- .$lake_name[1] 
        new_data
        }) %>%
        unnest(cols = c(interpolated_date_mid_AD, interpolated_error_of_age_sd, mid_cm, lake_name)) %>% ungroup()

# use Loess line to predict interpolated values (base/bottom of sample)
interpolated_dates_base <- pb  %>%
        group_by(lake_name) %>%
        do({
        loess_fit_mean <- loess(date_base_AD ~ mid_cm, data = .)
        new_data <- data.frame(mid_cm = seq(unique(.$min_bottom_cm_210Pb_dated)- 0.25, unique(.$max_bottom_cm_210Pb_dated) - 0.25, by = 0.5))
        new_data$interpolated_date_base_AD <- round(predict(loess_fit_mean, newdata = new_data),digits=2)
        new_data$lake_name <- .$lake_name[1] 
        new_data
        }) %>%
        unnest(cols = c(interpolated_date_base_AD, mid_cm, lake_name)) %>% ungroup()

# add in a column for the top sediment date (used to approximate the top of the interval for prism data)

# function to convert date to decimal year
decimal_year <- function(date) {
  year <- year(date)
  start_of_year <- ymd(paste0(year, "-01-01"))
  days_in_year <- ifelse(leap_year(date), 366, 365) # Account for leap years
  fractional_year <- as.numeric(difftime(date, start_of_year, units = "days")) / days_in_year
  year + fractional_year
} 

#make a dataframe that has the decimal year for each lake
date_cored <- pb %>%
  select(lake_name,date_cored) %>% distinct(.) %>%  mutate(decimal_year=decimal_year(dmy(date_cored))) %>% select(-date_cored)
        
# add it to the interpolated_dates_base dataframe
interpolated_dates_base <-left_join(interpolated_dates_base,date_cored, join_by(lake_name)); rm(date_cored)

interpolated_dates_base <- interpolated_dates_base %>% group_by(lake_name) %>% mutate(interpolated_date_top_AD = c(decimal_year[1],interpolated_date_base_AD[1:length(interpolated_date_base_AD)-1])) %>% select(-decimal_year)

rm(decimal_year) # remove this function

# join the two columns for the middle date and the base date
interpolated_dates<-inner_join(interpolated_dates_mid,interpolated_dates_base, by=join_by(lake_name,mid_cm)); rm(interpolated_dates_mid); rm(interpolated_dates_base)

# merge back the interpolated dates
pb <-left_join(pb,as.data.frame(interpolated_dates), by=join_by(lake_name, mid_cm)); rm(interpolated_dates)


# extrapolate using the DMAR
# calculate the average DMAR for the last three samples
average_DMAR <- pb %>%
  group_by(lake_name) %>%
  filter(is.na(DMAR_g_cm2_yr) == F) %>%
  reframe(mean_DMAR_g_cm2_yr_bottom3 = mean(tail(DMAR_g_cm2_yr, n = 3)),
        mean_error_DMAR_sd_bottom3 = mean(tail(error_DMAR_sd, n = 3)),
        mean_DMAR_g_cm2_yr_bottom3_max = mean_DMAR_g_cm2_yr_bottom3 + mean_error_DMAR_sd_bottom3,
        mean_DMAR_g_cm2_yr_bottom3_min = mean_DMAR_g_cm2_yr_bottom3 - mean_error_DMAR_sd_bottom3,
        bottom_interpolated_error= tail(interpolated_error_of_age_sd,n=1),
        bottom_measured_date_mid_AD = tail(date_mid_AD, n=1)) %>%
        mutate(mean_DMAR_g_cm2_yr_bottom3_min = ifelse(mean_DMAR_g_cm2_yr_bottom3_min<0,0.01,mean_DMAR_g_cm2_yr_bottom3_min)) 
# used error of DMAR 0.001 because range for this data is 0.00024 0.03320 and using 0 produced -Inf


# join back to the main dataframe
pb <- left_join(pb, average_DMAR, by=join_by(lake_name)); rm(average_DMAR)

# calculate the accumulation rate 
pb <- pb %>%
        mutate(thickness=bottom_cm-top_cm,
               # calculate the mean DMAR 
               accumulation_rate_cm_yr=ifelse(date_type=="extrapolated",mean_DMAR_g_cm2_yr_bottom3 /all_rho_g_cm3, NA),
               years_per_thickness=thickness/accumulation_rate_cm_yr,
               # calculate the mean error for DMAR 
               accumulation_rate_max_cm_yr=ifelse(date_type=="extrapolated",mean_DMAR_g_cm2_yr_bottom3_max /all_rho_g_cm3, NA),
               accumulation_rate_min_cm_yr=ifelse(date_type=="extrapolated",mean_DMAR_g_cm2_yr_bottom3_min /all_rho_g_cm3, NA),
               years_per_thickness_max=thickness/accumulation_rate_max_cm_yr,
               years_per_thickness_min=thickness/accumulation_rate_min_cm_yr)
        
extrapolated_dates <- pb %>% group_by(lake_name) %>% filter(date_type=="extrapolated") %>%
        mutate(cumulative_years = cumsum(years_per_thickness),
               extrapolated_date_mid_AD = round(bottom_measured_date_mid_AD - cumulative_years,digits=2),
               # using error of DMAR
               cumulative_years_max = cumsum(years_per_thickness_max),
               extrapolated_date_mid_AD_max = round(bottom_measured_date_mid_AD - cumulative_years_max,digits=2),
                cumulative_years_min = cumsum(years_per_thickness_min),
               extrapolated_date_mid_AD_min = round(bottom_measured_date_mid_AD - cumulative_years_min,digits=2))%>% 
        ungroup() %>%  select(sample_id,cumulative_years, extrapolated_date_mid_AD, cumulative_years_max, extrapolated_date_mid_AD_max,  cumulative_years_min, extrapolated_date_mid_AD_min)

# join back to the main dataframe
pb <- left_join(pb, extrapolated_dates, by=join_by(sample_id)); rm(extrapolated_dates)

# create one column for the measured, interpolated, and extrapolated dates
pb <- pb %>% 
        mutate(date_mid_AD_combo=case_when(date_type=="measured" ~ date_mid_AD,
                                           date_type=="extrapolated" ~ extrapolated_date_mid_AD,
                                           date_type=="interpolated" ~ interpolated_date_mid_AD),
               date_mid_AD_min_combo= case_when(date_type=="measured" ~ date_mid_AD - error_of_age_sd,
                                           date_type=="extrapolated" ~ extrapolated_date_mid_AD_min- bottom_interpolated_error,
                                           date_type=="interpolated" ~ interpolated_date_mid_AD - interpolated_error_of_age_sd),
               date_mid_AD_max_combo= case_when(date_type=="measured" ~ date_mid_AD + error_of_age_sd,
                                           date_type=="extrapolated" ~ extrapolated_date_mid_AD_max + bottom_interpolated_error,
                                           date_type=="interpolated" ~ interpolated_date_mid_AD + interpolated_error_of_age_sd))
pb <- as.data.frame(pb)

write.csv(pb,paste0(Sys.Date(),"_sample_data_dates.csv"))
rm(pb);rm(metadata)
```
I am thinking here that I should just not include that final value and see how this looks for Soldier lake


# 3. Plot 210Pb dates
```{r}
pb <- read.csv(list.files(pattern = "*_sample_data_dates.csv", full.names = TRUE), row.names = 1, header = TRUE)

# set colors for lakes
lake_colors <- c("Canyon" = "#b9ceac", "Eyrie" = "darkolivegreen3", "Lightning" = "darkolivegreen", 
                 "Black Rock" = "#FFBBFF", "Footprint" = "#CD96CD", "Lost" = "#9696CD", "Scott" = "#5D478B", 
                 "Bullfrog"= "#BFEFFF", "Middle Gaylor"= "#8EE5EE", "Soldier"="#436EEE", "Skelton"="#63B8FF", 
                 "Crescent"= "#EE6363", "Louise"="#FFA07A","Monogram"="#EE30A7")


lk <- pb %>%
  mutate(lake_name = factor(lake_name, levels = names(lake_colors))) %>%
  arrange(lake_name, mid_cm) 

# make dataframes of polygons
poly <- data.frame()
for (lake in unique(lk$lake_name)) {
  lk_sub <- lk %>% filter(lake_name == lake)
  poly <- rbind(poly, data.frame(
    x = c(rev(lk_sub$date_mid_AD_min_combo), lk_sub$date_mid_AD_max_combo),
    y = c(rev(lk_sub$mid_cm), lk_sub$mid_cm),
    lake_name = lake
  ))
}
poly <- poly %>% filter(!(lake_name == "Soldier" & y %in% c(1.25, 0.5))) # remove top Soldier samples without data


poly$lake_name <- factor(poly$lake_name, levels = names(lake_colors))

  custom_theme <- function() {
  theme_bw() +
    theme(
      text = element_text(color = "black", size = 13),
      axis.text = element_text(color = "black",size = 13),
      axis.title = element_text(color = "black",size = 13),
      axis.title.x = element_text(margin = margin(t = 10)),
      axis.title.y = element_text(margin = margin(r = 10)),
      plot.title = element_text(hjust = 0.5, size = 14, face = "bold"),
      plot.subtitle = element_text(hjust = 0.5, size = 12),
      legend.text = element_text(size = 13),
      legend.title = element_text(size = 13,hjust = 0.5),
      strip.text = element_text(size = 13, color = "black"))}

ggplot() +
 geom_polygon(data = poly, aes(x = x, y = y, fill = lake_name), alpha = 0.5)+
  geom_line(data = lk, aes(x = date_mid_AD_combo, y = mid_cm, color = lake_name), size = 1.5) +
  geom_point(data = filter(lk, date_type == "measured"), aes(x = date_mid_AD_combo, y = mid_cm), 
             color = "black", size = 2) +
  facet_wrap2(~ lake_name, ncol=7, strip.position = "top", strip=strip_themed(background_x = elem_list_rect(fill = lake_colors))) +
  labs(x = "Date (Common Era)", y = "Depth (cm)")+
 scale_y_reverse(limits = c(30, 0), breaks = seq(0, 30, 5)) +
 custom_theme()+
scale_color_manual(values = lake_colors) +
  scale_fill_manual(values = lake_colors) +
          theme(panel.grid.minor = element_blank(),
                panel.grid.major = element_blank(),
        strip.text.x = element_text(size = 12),
        axis.text.x = element_text(angle = 60, vjust = 1, hjust = 1, color = "black", size = 12),
        axis.text.y = element_text(color = "black", size = 12),
        axis.title = element_text(size = 12),
        legend.position = "none") +
         coord_cartesian(xlim = c(0, 2022)) +
        geom_vline(data=lk[,names(lk) %in%c("lake_name","first_stocked_yr")] %>% unique(.) %>%  filter(complete.cases(.)), aes(xintercept=first_stocked_yr), color="black",linewidth=0.75)

ggsave("3_Figures/FigX_210Pb_dates.png",height=5, width=14, units="in")

```
Notes: min(poly$x)  -975.239, its monogram because it is so deep (38 cm), so cut off here at 30 cm while still reasonable

# 4. Create phyloseq
```{r}

```

# 5. NMDS
This is a quick plotting with chatGPT, go back to old code and do in base R
```{r}
#nmds_V7 <- ps_V7_norm %>% ordinate(., method = "NMDS", distance = "bray")
nmds_points <- as.data.frame(nmds_V7$points)  # Extract NMDS points
nmds_points$sample_id <- rownames(nmds_points)  # Add sample IDs
nmds_meta <- merge(nmds_points, metadata, by = "sample_id")
lake_colors <- c("Canyon" = "#b9ceac", "Eyrie" = "darkolivegreen3", "Lightning" = "darkolivegreen", 
                 "Black Rock" = "#FFBBFF", "Footprint" = "#CD96CD", "Lost" = "#9696CD", 
                 "Scott" = "#5D478B", "Bullfrog" = "#BFEFFF", "Middle Gaylor" = "#8EE5EE", 
                 "Soldier" = "#436EEE", "Skelton" = "#63B8FF", "Crescent" = "#EE6363", 
                 "Louise" = "#FFA07A", "Monogram" = "#EE30A7")
                 
nmds_meta <- nmds_meta %>%
  mutate(lake_name = factor(lake_name, levels = names(lake_colors)))  # Ensure lake_name is a factor

nmds_meta<-nmds_meta %>%
        arrange(mountain_range, fish, lake_name, bottom_cm) %>%
        group_by(lake_name)%>%
        mutate(fish_present_sample = ifelse(first_stocked_yr>=date_mid_AD_combo | is.na(first_stocked_yr) ==T, "fishless","fish")) %>% ungroup()


ggplot(nmds_meta, aes(x = MDS1, y = MDS2, fill = lake_name, shape = fish_present_sample, color = fish_present_sample)) +
  geom_point(size = 5, alpha = 0.8, stroke = 1.2) +  # Plot points with black outline for fish
  scale_fill_manual(values = lake_colors) +  # Use lake colors for the fill
  scale_color_manual(values = c("fish" = "black", "fishless" = NA)) +  # Black outline for fish, no outline for fishless
  scale_shape_manual(values = c("fish" = 21, "fishless" = 21)) +  # Use shape 21 for filled points
  labs(x = "NMDS1", y = "NMDS2", fill = "Lake name", color = "Fish status", shape = "Fish status") +  # Legend labels
  guides(color = guide_legend(override.aes = list(fill = "black", size = 5)),  # Ensure black outline for fish in the legend
         fill = guide_legend(override.aes = list(color = "black", shape = 21))) +  # Keep lake colors for fill
  theme_minimal() +  # Minimal theme for a clean plot
  theme(legend.position = "right")  # Legend on the right


ggsave("3_Figures/FigX_NMDS_V7.png",height=6, width=8, units="in")

nmds_V9 <- ps_V9_norm %>% ordinate(., method = "NMDS", distance = "bray")
nmds_points <- as.data.frame(nmds_V9$points)  # Extract NMDS points
nmds_points$sample_id <- rownames(nmds_points)  # Add sample IDs
nmds_meta <- merge(nmds_points, metadata, by = "sample_id")
lake_colors <- c("Canyon" = "#b9ceac", "Eyrie" = "darkolivegreen3", "Lightning" = "darkolivegreen", 
                 "Black Rock" = "#FFBBFF", "Footprint" = "#CD96CD", "Lost" = "#9696CD", 
                 "Scott" = "#5D478B", "Bullfrog" = "#BFEFFF", "Middle Gaylor" = "#8EE5EE", 
                 "Soldier" = "#436EEE", "Skelton" = "#63B8FF", "Crescent" = "#EE6363", 
                 "Louise" = "#FFA07A", "Monogram" = "#EE30A7")
                 
nmds_meta <- nmds_meta %>%
  mutate(lake_name = factor(lake_name, levels = names(lake_colors)))  # Ensure lake_name is a factor

nmds_meta<-nmds_meta %>%
        arrange(mountain_range, fish, lake_name, bottom_cm) %>%
        group_by(lake_name)%>%
        mutate(fish_present_sample = ifelse(first_stocked_yr>=date_mid_AD_combo | is.na(first_stocked_yr) ==T, "fishless","fish")) %>% ungroup()


ggplot(nmds_meta, aes(x = MDS1, y = MDS2, fill = lake_name, shape = fish_present_sample, color = fish_present_sample)) +
  geom_point(size = 5, alpha = 0.8, stroke = 1.2) +  # Plot points with black outline for fish
  scale_fill_manual(values = lake_colors) +  # Use lake colors for the fill
  scale_color_manual(values = c("fish" = "black", "fishless" = NA)) +  # Black outline for fish, no outline for fishless
  scale_shape_manual(values = c("fish" = 21, "fishless" = 21)) +  # Use shape 21 for filled points
  labs(x = "NMDS1", y = "NMDS2", fill = "Lake name", color = "Fish status", shape = "Fish status") +  # Legend labels
  guides(color = guide_legend(override.aes = list(fill = "black", size = 5)),  # Ensure black outline for fish in the legend
         fill = guide_legend(override.aes = list(color = "black", shape = 21))) +  # Keep lake colors for fill
  theme_minimal() +  # Minimal theme for a clean plot
  theme(legend.position = "right")  # Legend on the right

ggsave("3_Figures/FigX_NMDS_V9.png",height=6, width=8, units="in")

```
issue - there is an NA here

# 6. Isotope 
```{r}

# Arrange metadata by mountain_range, fish, lake_name, and bottom_cm
meta <- metadata %>% filter(isotope_run=="CN") %>%
  arrange(state, fish, lake_name, bottom_cm) %>%
  mutate(lake_name = factor(lake_name, levels = unique(lake_name)))  # Set lake_name as a factor based on the desired order

# Plot with facet_wrap ordered by lake_name
ggplot(data = meta, aes(x = date_mid_AD_combo, y = d15N)) +
  geom_line() + 
  geom_point() + 
  geom_errorbar(aes(ymin = d15N - 0.4, ymax = d15N + 0.4)) +  # Add error bars
  facet_wrap(~lake_name, nrow=2, ncol=7) +  
  labs(x = "Year (Common Era)") + 
  custom_theme()  

ggsave("3_Figures/FigX_d15N.png",height=5, width=16, units="in")


ggplot(data = meta, aes(x = date_mid_AD_combo, y =d13C)) +
  geom_line() + 
  geom_point() + 
  facet_wrap(~lake_name, nrow=2, ncol=7) +  
  labs(x = "Year (Common Era)") +  
  custom_theme()  #

ggsave("3_Figures/FigX_d13C.png",height=5, width=16, units="in")


ggplot(data = meta, aes(x = date_mid_AD_combo, y =CN_ratio)) +
  geom_line() + 
  geom_point() + 
  facet_wrap(~lake_name, nrow=2, ncol=7) +  
  labs(x = "Year (Common Era)") +  
  custom_theme()  #

ggsave("3_Figures/FigX_CN.png",height=5, width=16, units="in")

ggplot(data = meta, aes(x = date_mid_AD_combo, y =C_perc)) +
  geom_line() + 
  geom_point() + 
  facet_wrap(~lake_name, nrow=2, ncol=7) +  
  labs(x = "Year (Common Era)") + 
  custom_theme()  

ggsave("3_Figures/FigX_C_perc.png",height=5, width=16, units="in")

ggplot(data = meta, aes(x = mean_C.H, y =C_perc)) +
  geom_point() + 
  facet_wrap(~lake_name, nrow=2, ncol=7) +  
  labs(x = "Year (Common Era)") +  
  custom_theme()  

ggsave("3_Figures/FigX_C_perc_vs_CH.png",height=5, width=16, units="in")


```

# 7. Biogenic silica
```{r}
# Arrange metadata by mountain_range, fish, lake_name, and bottom_cm
meta <- metadata %>% filter(is.na(mean_BiSi)==F) %>%
  arrange(state, fish, lake_name, bottom_cm) %>%
  mutate(lake_name = factor(lake_name, levels = unique(lake_name)))  


ggplot(data = meta, aes(x = date_mid_AD_combo, y =mean_BiSi)) +
  facet_wrap(~lake_name, nrow=2, ncol=7) + 
        custom_theme() +
        geom_ribbon(aes(ymin = mean_BiSi - (mean_range_BiSi / 2), 
                        ymax = mean_BiSi + (mean_range_BiSi / 2)),
                    fill="orange") +
        geom_line() + 
        geom_point() +
        labs(x = "Year (Common Era)",
             y="BiSi")

ggsave("3_Figures/FigX_BiSi_mean_error.png",height=5, width=16, units="in")

ggplot(data = meta, aes(x = date_mid_AD_combo, y =mean_BiSi)) +
  facet_wrap(~lake_name, nrow=2, ncol=7) + 
        custom_theme() +
        geom_ribbon(aes(ymin = mean_BiSi - (max_range_BiSi / 2), 
                        ymax = mean_BiSi + (max_range_BiSi / 2)),
                    fill="orange") +
        geom_line() + 
        geom_point() +
        labs(x = "Year (Common Era)",
             y="BiSi")

ggsave("3_Figures/FigX_BiSi_max_error.png",height=5, width=16, units="in")



ggplot(data = meta, aes(x = date_mid_AD_combo, y =mean_C.H)) +
  facet_wrap(~lake_name, nrow=2, ncol=7) + 
        custom_theme() +
        geom_ribbon(aes(ymin = mean_C.H - (mean_range_C.H / 2), 
                        ymax = mean_C.H + (mean_range_C.H / 2)),
                    fill="orange") +
        geom_line() + 
        geom_point() +
        labs(x = "Year (Common Era)",
             y="C-H")

ggsave("3_Figures/FigX_C.H_mean_error.png",height=5, width=16, units="in")

ggplot(data = meta, aes(x = date_mid_AD_combo, y =mean_C.H)) +
  facet_wrap(~lake_name, nrow=2, ncol=7) + 
        custom_theme() +
        geom_ribbon(aes(ymin = mean_C.H - (max_range_C.H / 2), 
                        ymax = mean_C.H + (max_range_C.H / 2)),
                    fill="orange") +
        geom_line() + 
        geom_point() +
        labs(x = "Year (Common Era)",
             y="C-H")

ggsave("3_Figures/FigX_CH_max_error.png",height=5, width=16, units="in")

```
- normalize for dry mass
- normalize for clastics (1-organics)
- see if it relates to the percent of diatoms and crysophytes
